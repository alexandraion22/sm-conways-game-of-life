Elapsed Time: 28.974s
    IPC: 1.836
    Average CPU Frequency: 3.392 GHz
Logical Core Utilization: 3.1% (0.981 out of 32)
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization. Consider improving physical core utilization as the first step
 | and then look at opportunities to utilize logical cores, which in some cases
 | can improve processor throughput and overall performance of multi-threaded
 | applications.
 |
    Physical Core Utilization: 6.1% (0.980 out of 16)
     | The metric value is low, which may signal a poor physical CPU cores
     | utilization caused by:
     |     - load imbalance
     |     - threading runtime overhead
     |     - contended synchronization
     |     - thread/process underutilization
     |     - incorrect affinity that utilizes logical cores instead of physical
     |       cores
     | Run the HPC Performance Characterization analysis to estimate the
     | efficiency of MPI and OpenMP parallelism or run the Locks and Waits
     | analysis to identify parallel bottlenecks for other parallel runtimes.
     |
Microarchitecture Usage: 45.8% of Pipeline Slots
 | You code efficiency on this platform is too low.
 | 
 | Possible cause: memory stalls, instruction starvation, branch misprediction
 | or long latency instructions.
 | 
 | Next steps: Run Microarchitecture Exploration analysis to identify the cause
 | of the low microarchitecture usage efficiency.
 |
    Retiring: 45.8% of Pipeline Slots
    Front-End Bound: 4.2% of Pipeline Slots
        Front-End Latency: 2.1% of Pipeline Slots
        Front-End Bandwidth: 2.1% of Pipeline Slots
    Bad Speculation: 22.1% of Pipeline Slots
     | A significant proportion of pipeline slots containing useful work are
     | being cancelled. This can be caused by mispredicting branches or by
     | machine clears. Note that this metric value may be highlighted due to
     | Branch Resteers issue.
     |
    Back-End Bound: 27.9% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 13.5% of Pipeline Slots
            L1 Bound: 8.0% of Clockticks
            L2 Bound: N/A with HT on
            L3 Bound: N/A with HT on
            DRAM Bound: N/A with HT on
                Memory Bandwidth: N/A with HT on
                Memory Latency: N/A with HT on
            Store Bound: 9.4% of Clockticks
        Core Bound: 14.4% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Port Utilization: 18.6% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 42.6% of Clockticks
                 | CPU executed no uOps on any execution port during a
                 | significant fraction of cycles. Long-latency instructions
                 | like divides may contribute to this issue. Check the Assembly
                 | view and Appendix C in the Optimization Guide to identify
                 | instructions with 5 or more cycles latency.
                 |
                Cycles of 1 Port Utilized: 5.8% of Clockticks
                Cycles of 2 Ports Utilized: 6.2% of Clockticks
                Cycles of 3+ Ports Utilized: 30.5% of Clockticks
Memory Bound: 13.5% of Pipeline Slots
    Cache Bound: 8.0% of Clockticks
    DRAM Bound: N/A with HT on
    NUMA: % of Remote Accesses: 0.4%
Collection and Platform Info
    Application Command Line: ./game_of_life "input_10000_x_20000.txt" 
    Operating System: 4.18.0-481.el8.x86_64 DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.3 LTS"
    Computer Name: haswell-wn30.grid.pub.ro
    Result Size: 3.6 MB 
    Collection start time: 17:03:09 14/01/2024 UTC
    Collection stop time: 17:03:38 14/01/2024 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.600 GHz
        Logical CPU Count: 32
        LLC size: 21.0 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

Recommendations:
    Hotspots: Start with Hotspots analysis to understand the efficiency of your algorithm.
     | Use Hotspots analysis to identify the most time consuming functions.
     | Drill down to see the time spent on every line of code.
    Threading: There is poor utilization of logical CPU cores (3.1%) in your application.
     |  Use Threading to explore more opportunities to increase parallelism in
     | your application.
    Microarchitecture Exploration: There is low microarchitecture usage (45.8%) of available hardware resources.
     | Run Microarchitecture Exploration analysis to analyze CPU
     | microarchitecture bottlenecks that can affect application performance.
    HPC Performance Characterization: Your application uses MPI. Start with Application Performance Snapshot to get MPI-specific insights for your application.
     | Use HPC Performance Characterization analysis to examine the performance
     | of compute-intensive applications. Understand CPU/GPU utilization and get
     | information about OpenMP efficiency, memory access, and vectorization.

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
